<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://blogosfair.github.io/How-much-meta-learning-is-in-image-to-image-translation/feed.xml" rel="self" type="application/atom+xml"/><link href="https://blogosfair.github.io/How-much-meta-learning-is-in-image-to-image-translation/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-04-25T10:19:51+00:00</updated><id>https://blogosfair.github.io/How-much-meta-learning-is-in-image-to-image-translation/feed.xml</id><title type="html">ICLR Blogposts 2023 (staging)</title><subtitle>Staging website for the 2023 ICLR Blogposts track </subtitle><entry><title type="html">Sample Blog Post</title><link href="https://blogosfair.github.io/How-much-meta-learning-is-in-image-to-image-translation/blog/2022/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://blogosfair.github.io/How-much-meta-learning-is-in-image-to-image-translation/blog/2022/distill-example</id><content type="html" xml:base="https://blogosfair.github.io/How-much-meta-learning-is-in-image-to-image-translation/blog/2022/distill-example/"><![CDATA[<h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2022-12-01-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/iclr-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2023-05-01-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/9-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/7-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/8-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/10-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/11-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/12-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/7-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="s">'Latitude'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="s">'Longitude'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="s">'Magnitude'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="s">"stamen-terrain"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="s">'./assets/html/2022-12-01-distill-example/plotly_demo_1.html'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2022-12-01-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/How-much-meta-learning-is-in-image-to-image-translation/assets/html/2022-12-01-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the fist time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1682417995708" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1682417995708 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1682417995708 .node circle,#mermaid-1682417995708 .node ellipse,#mermaid-1682417995708 .node polygon,#mermaid-1682417995708 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1682417995708 .node.clickable{cursor:pointer}#mermaid-1682417995708 .arrowheadPath{fill:#333}#mermaid-1682417995708 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1682417995708 .edgeLabel{background-color:#e8e8e8}#mermaid-1682417995708 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1682417995708 .cluster text{fill:#333}#mermaid-1682417995708 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1682417995708 .actor{stroke:#ccf;fill:#ececff}#mermaid-1682417995708 text.actor{fill:#000;stroke:none}#mermaid-1682417995708 .actor-line{stroke:grey}#mermaid-1682417995708 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1682417995708 .messageLine0,#mermaid-1682417995708 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1682417995708 #arrowhead{fill:#333}#mermaid-1682417995708 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1682417995708 .messageText{fill:#333;stroke:none}#mermaid-1682417995708 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1682417995708 .labelText,#mermaid-1682417995708 .loopText{fill:#000;stroke:none}#mermaid-1682417995708 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1682417995708 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1682417995708 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1682417995708 .section{stroke:none;opacity:.2}#mermaid-1682417995708 .section0{fill:rgba(102,102,255,.49)}#mermaid-1682417995708 .section2{fill:#fff400}#mermaid-1682417995708 .section1,#mermaid-1682417995708 .section3{fill:#fff;opacity:.2}#mermaid-1682417995708 .sectionTitle0,#mermaid-1682417995708 .sectionTitle1,#mermaid-1682417995708 .sectionTitle2,#mermaid-1682417995708 .sectionTitle3{fill:#333}#mermaid-1682417995708 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1682417995708 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1682417995708 .grid path{stroke-width:0}#mermaid-1682417995708 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1682417995708 .task{stroke-width:2}#mermaid-1682417995708 .taskText{text-anchor:middle;font-size:11px}#mermaid-1682417995708 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1682417995708 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1682417995708 .taskText0,#mermaid-1682417995708 .taskText1,#mermaid-1682417995708 .taskText2,#mermaid-1682417995708 .taskText3{fill:#fff}#mermaid-1682417995708 .task0,#mermaid-1682417995708 .task1,#mermaid-1682417995708 .task2,#mermaid-1682417995708 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1682417995708 .taskTextOutside0,#mermaid-1682417995708 .taskTextOutside1,#mermaid-1682417995708 .taskTextOutside2,#mermaid-1682417995708 .taskTextOutside3{fill:#000}#mermaid-1682417995708 .active0,#mermaid-1682417995708 .active1,#mermaid-1682417995708 .active2,#mermaid-1682417995708 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1682417995708 .activeText0,#mermaid-1682417995708 .activeText1,#mermaid-1682417995708 .activeText2,#mermaid-1682417995708 .activeText3{fill:#000!important}#mermaid-1682417995708 .done0,#mermaid-1682417995708 .done1,#mermaid-1682417995708 .done2,#mermaid-1682417995708 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1682417995708 .doneText0,#mermaid-1682417995708 .doneText1,#mermaid-1682417995708 .doneText2,#mermaid-1682417995708 .doneText3{fill:#000!important}#mermaid-1682417995708 .crit0,#mermaid-1682417995708 .crit1,#mermaid-1682417995708 .crit2,#mermaid-1682417995708 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1682417995708 .activeCrit0,#mermaid-1682417995708 .activeCrit1,#mermaid-1682417995708 .activeCrit2,#mermaid-1682417995708 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1682417995708 .doneCrit0,#mermaid-1682417995708 .doneCrit1,#mermaid-1682417995708 .doneCrit2,#mermaid-1682417995708 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1682417995708 .activeCritText0,#mermaid-1682417995708 .activeCritText1,#mermaid-1682417995708 .activeCritText2,#mermaid-1682417995708 .activeCritText3,#mermaid-1682417995708 .doneCritText0,#mermaid-1682417995708 .doneCritText1,#mermaid-1682417995708 .doneCritText2,#mermaid-1682417995708 .doneCritText3{fill:#000!important}#mermaid-1682417995708 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1682417995708 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1682417995708 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1682417995708 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1682417995708 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1682417995708 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1682417995708 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1682417995708 #compositionEnd,#mermaid-1682417995708 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1682417995708 #aggregationEnd,#mermaid-1682417995708 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1682417995708 #dependencyEnd,#mermaid-1682417995708 #dependencyStart,#mermaid-1682417995708 #extensionEnd,#mermaid-1682417995708 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1682417995708 .branch-label,#mermaid-1682417995708 .commit-id,#mermaid-1682417995708 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1682417995708{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> <ul> <li>Unordered list can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="s">"Python syntax highlighting"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.]]></summary></entry><entry><title type="html">How much meta-learning is in image-to-image translation?</title><link href="https://blogosfair.github.io/How-much-meta-learning-is-in-image-to-image-translation/blog/2022/how-much-meta-learning-is-in-image-to-image-translation/" rel="alternate" type="text/html" title="How much meta-learning is in image-to-image translation?"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://blogosfair.github.io/How-much-meta-learning-is-in-image-to-image-translation/blog/2022/how-much-meta-learning-is-in-image-to-image-translation</id><content type="html" xml:base="https://blogosfair.github.io/How-much-meta-learning-is-in-image-to-image-translation/blog/2022/how-much-meta-learning-is-in-image-to-image-translation/"><![CDATA[<p>At the last ICLR conference, Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> presented work showing that CNNs do not transfer information between classes of a classification task.</p> <ul> <li>Allan Zhou, Fahim Tajwar, Alexander Robey, Tom Knowles, George J. Pappas, Hamed Hassani, Chelsea Finn [ICLR, 2022] Do Deep Networks Transfer Invariances Across Classes?<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite></li> </ul> <p>Here is a quick summary of their findings: If we train a Convolutional Neural Net (CNN) to classify animals on a set of randomly brightened and darkened images of cats and dogs, it will learn to ignore the scene’s brightness. We say that the CNN learned that classification is <strong>invariant</strong> to the <strong>nuisance transformation</strong> of randomly changing the brightness of an image. We now add a set of leopards to the training data, but fewer examples of them (they are hard to photograph) than we have cats and dogs. However, we keep using the same random transformations. The training set thus becomes <strong>class-imbalanced</strong>.</p> <p>We might expect a sophisticated learner to look at the entire dataset, recognize the random brightness modifications across all species of animal and henceforth ignore brightness when making predictions. If this applied to our experiment, the CNN would be similarly good at ignoring lighting variations on all animals. Furthermore, we would expect the CNN to become more competent at ignoring lighting variations in proportion to <strong>the total amount of images</strong>, irrespective of which animal they depict.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/CONCEPTUAL_DIAGRAM.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/CONCEPTUAL_DIAGRAM.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/CONCEPTUAL_DIAGRAM.svg-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/CONCEPTUAL_DIAGRAM.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> show that a CNN does not behave like this: When using a CNN on a <strong>class-imbalanced</strong> classification task with random nuisance transformations, the CNNs invariance to the transformation is proportional to the size of the training set <strong>for each class</strong>. This finding suggests CNNs don’t <strong>transfer invariance</strong> between classes when learning such a classification task.</p> <p>However, there is a solution: Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> use an Image-to-Image translation architecture called MUNIT<d-cite key="DBLP:conf/eccv/HuangLBK18"></d-cite> to learn the transformations and generate additional data from which the CNN can learn the invariance separately for each class. Thus, the invariance to nuisance transformations is transferred <strong>generatively</strong>. They call this method <strong>Generative Invariance Transfer (GIT)</strong>.</p> <p><strong>So why is this an interesting result?</strong></p> <p>In the field of machine learning many have dreamed for a long time<d-cite key="schmidhuber:1987:srl"></d-cite><d-cite key="DBLP:books/sp/98/ThrunP98"></d-cite> of a learner that, having learned a number of tasks can adapt to new tasks with little to no extra training - a learner that has learned to learn, a meta-learner. Yet, specialized meta-learners <d-cite key="DBLP:conf/icml/FinnAL17"></d-cite><d-cite key="NIPS2017_cb8da676"></d-cite><d-cite key="NIPS2016_90e13578"></d-cite><d-cite key="sung2018learning"></d-cite> struggled to outperform baseline methods<d-cite key="DBLP:journals/corr/abs-2104-02638"></d-cite><d-cite key="DBLP:journals/corr/abs-1904-04232"></d-cite>, arguably due to high computational requirements<d-cite key="nichol2018first"></d-cite> and few large scale datasets<d-cite key="triantafillou2019meta"></d-cite>. We believe this to be caused by a too-narrow conception of what constitutes meta-learning. We argue that:</p> <ul> <li>In contradiction to recent definitions of meta-learning, the experiment described above is a meta-learning experiment.</li> <li>MUNIT is related to contemporary meta-learning methods and a meta-learner.</li> <li>These two findings point to a too-narrow conception of meta-learning in the recent literature. A wider conception based on mutual information could lead to interesting future work.</li> </ul> <p>Before we proceed to the main post, let’s clarify some definitions. If you are already familiar with the subject, you may skip this part. If you have only a vague notion of contemporary meta-learning you will be able to follow the article anyway. However, if you want to know more, <a href="https://interactive-maml.github.io/">here</a> is a gentle introduction to MAML, one of the most popular methods.</p> <details> <summary><b> Definition: Class-Imbalanced Classification</b></summary> <br/> <p> In many real-world classification datasets, the number of examples for each class varies. <b>Class-imbalanced classification</b> refers to classification on datasets where the frequencies of class labels vary significantly. </p> <p> It is generally more difficult for a neural network to learn to classify classes with fewer examples <d-cite key="5128907"></d-cite><d-cite key="10.1117/12.2228523"></d-cite>. However, it is often important to perform well on all classes, regardless of their frequency in the dataset. If we train a model to classify a dataset of different skin tumors, most examples may be benign. Still, it is crucial to identify the rare, malignant ones. Experiment design, including training and evaluation methods must therefore be adjusted when using class-imbalanced data. (see Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> section 3.1) </p> <br/> </details> <details> <summary><b> Definition: Nuisance Transformation &amp; Transformation Invariance</b></summary> <br/> <p> Transformations are alterations of data. In the context of image classification, <b>nuisance transformations</b> are alterations that do not affect the class labels of the data. A model is said to be invariant to a <b>nuisance transformation</b> if it can successfully ignore the transformation when predicting a class label. </p> We can formally define a <b>nuisance transformation</b> <p> $$T(\cdot |x)$$ </p> <p> as a distribution over transformation functions. An example of a <b>nuisance transformation</b> might be a distribution over rotation matrices of different angles, or lighting transformations with different exposure values. By definition, <b>nuisance transformations</b> have no impact on class labels $y$, only on data $x$. A perfectly <b>transformation-invariant</b> classifier would thus completely ignore them, i.e., </p> <p> $$ \hat{P}_w(y = j|x) = \hat{P}_w(y = j|x'), \; x' \sim T(\cdot |x). $$ </p> <p> (see Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> section 3.1) </p> </details> <h2 id="a-closer-look-at-the-experiment">A closer look at the experiment</h2> <p>Let’s take a more detailed look at the experiment Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> conducted:</p> <p>Zhou et al. take a dataset, e.g., CIFAR-100, then apply a nuisance transformation, for example, random rotation, background intensity, or dilation and erosion. They then remove samples from some classes until the distribution of class sizes follows <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipf’s law</a> with parameter 2.0 and a minimum class size of 5. The test set remains balanced, i.e., all test classes have the same number of samples. They then train a CNN model - for example, a ResNet - on this imbalanced and transformed training data.</p> <p>To measure the invariance of the trained model to the applied transformation Zhou et al. use the empirical <a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> between the predictions on the untransformed test set and the transformed test set of each class.</p> <p>If the learner is invariant to the transformation, the predicted probability distribution over class labels should be identical for the transformed and untransformed images. In that case, the KLD should be zero and greater than zero otherwise. The higher the expected KL-divergence, the more the applied transformation impacts the network’s predictions.</p> <p>The result: eKLD falls with class size. This implies that the CNN does not learn that there are the same nuisance transformations on all images and therefore does not transfer this knowledge to the classes with less training data. A CNN learns invariance <strong>separately for each class</strong>(see also Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> section 3.2).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="how-is-this-a-meta-learning-experiment">How is this a meta-learning experiment?</h2> <p>You might think this is a cool experiment, but how is it related to meta-learning?</p> <p>And, indeed, in contemporary literature meta-learning is often conceived of as learning multiple tasks. In an much-cited 2022 survey, Hosepdales et al. write:</p> <blockquote> <p>Meta-learning is most commonly understood as learning to learn; the process of improving a learning algorithm over multiple learning episodes. In contrast, conventional ML improves model predictions over multiple data instances. <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite></p> </blockquote> <p>In another popular survey Vanschoren [2018] says:</p> <blockquote> <p>First, we need to collect meta-data that describe prior learning tasks and previously learned models. They comprise the exact algorithm configurations used to train the models, including hyperparameter settings, pipeline compositions and/or network architectures, the resulting model evaluations, such as accuracy and training time, the learned model parameters, such as the trained weights of a neural net, as well as measurable properties of the task itself, also known as meta-features.<d-cite key="vanschoren2018meta"></d-cite></p> </blockquote> <p>Francheschi et al. [2018] basically equate meta-learning (ML) with <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">hyperparameter optimization</a> (HO):</p> <blockquote> <p>[…] both HO and ML essentially boil down to nesting two search problems: at the inner level we seek a good hypothesis (as in standard supervised learning) while at the outer level we seek a good configuration (including a good hypothesis space) where the inner search takes place.<d-cite key="DBLP:conf/icml/FranceschiFSGP18"></d-cite></p> </blockquote> <p>This perspective on meta-learning seems to indicate that “true” meta-learning requires a rigid structure of multiple discrete tasks that is optimized over. However, in the invariance transfer setting we neither have multiple learning episodes, i.e., we learn over multiple data instances, nor any “meta-features”. Also, adding a class to the dataset does not exactly constitute a new “task”, even though knowledge of the nuisance transform is applicable.</p> <p>So is Zhou et al.’s experiment no meta-learning after all?</p> <p>Let’s look at one of the original papers on meta-learning. In the 1998 book “Learning to learn” Sebastian Thrun &amp; Lorien Pratt define an algorithm as capable of “Learning to learn” if it improves its performance in proportion to the number of tasks it is exposed to:</p> <blockquote> <p>an algorithm is said to learn to learn if its performance at each task improves with experience and with the number of tasks. Put differently, a learning algorithm whose performance does not depend on the number of learning tasks, which hence would not benefit from the presence of other learning tasks, is not said to learn to learn <d-cite key="DBLP:books/sp/98/ThrunP98"></d-cite></p> </blockquote> <p>Now this seems a much looser definition. How might this apply to the experiment just outlined? In the introduction, we thought about how a sophisticated learner might handle a dataset like the one described in the last section. We said that a sophisticated learner would learn that the nuisance transformations are applied uniformly <strong>to all classes</strong>. Therefore, if we added more classes to the dataset, the learner would become <strong>more invariant</strong> to the transformations because we expose it to more examples of them. Since this is part of the classification task <strong>for each class</strong>, the learner should, everything else being equal, become better at classification, especially on classes with few training examples. To see this, we must think of the multi-classification task not as a single task but as multiple mappings from image features to activations that must be learned, as a set of binary classification tasks. Thrun and Pratt continue:</p> <blockquote> <p>For an algorithm to fit this definition, some kind of <em>transfer</em> must occur between multiple tasks that must have a positive impact on expected task-performance <d-cite key="DBLP:books/sp/98/ThrunP98"></d-cite>.</p> </blockquote> <p>This transfer is what Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> tried to measure. There is some meta-information learnable across several tasks, in our case, the transformation distribution across many binary classification tasks. If a learner can learn this meta-information and transfer it to each new task it has “learned to learn”; it is a meta-learner. The goal of Zhou et al.’s <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiment was to see whether this transfer takes place. Thus, arguably, it is a meta-learning experiment.</p> <h2 id="generative-invariance-transfer">Generative Invariance Transfer</h2> <p>Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> don’t stop there. They show that using the MUNIT (Multimodal Unsupervised image-to-image Translation)<d-cite key="DBLP:conf/eccv/HuangLBK18"></d-cite> architecture, they can learn the nuisance transformations applied to the dataset and generate additional training samples for the classes with few samples, improving transformation invariance there. They call this Generative invariance transfer (GIT). Let’s take a closer look:</p> <p>MUNIT networks are capable of performing image-to-image translation, which means that they can translate an image from one domain, such as pictures of leopards, into another domain, such as pictures of house cats. The translated image should look like a real house cat while still resembling the original leopard image. For instance, if the leopard in the original image has its eyes closed, the translated image should contain a house cat with closed eyes. Eye state is a feature present in both domains, so a good translator should not alter it. On the other hand, a leopard’s fur is yellow and spotted, while a house cat’s fur can be white, black, grey, or brown. To make the translated images indistinguishable from real house cats, the translator must thus replace leopard fur with house cat fur.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>MUNIT networks learn to perform translations by correctly distinguishing the domain-agnostic features (such as eye state) from the domain-specific features (such as the distribution of fur color). They embed an image into two latent spaces: a content space that encodes the domain-agnostic features and a style space that encodes the domain-specific features (see figure above).</p> <p>To transform a leopard into a house cat, we can encode the leopard into a content and a style code, discard the leopard-specific style code, randomly select a cat-specific style code, and assemble a house cat image that looks similar by combining the leopard’s content code with the randomly chosen cat style code (see figure below).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> modify the process of using MUNIT to transfer images between domains. They do not use MUNIT to translate images <strong>between</strong> domains but <strong>within</strong> a domain. The MUNIT network exchanges the style code of an image with another style code of the same domain. For example, if the domain is house cats, the MUNIT network might translate a grey house cat into a black one. The learning task in this single-domain application of MUNIT is to decompose example-agnostic content features from example-specific style features so that the translated images still look like house cats. For example, fur color is a valid style feature for translating within the ‘house cat’ domain because every house cat has a fur color. A translator only switching fur color is hard to detect.</p> <p>However, if the domain included house cats <strong>and apples</strong>, fur color is not a valid style feature. If it was, the translator might translate fur color on an apple and give it black fur, which would look suspiciously out of place. Whatever house cats and apples have in common - maybe their position or size in the frame - would be a valid style feature. We would expect an intra-domain translator on an apples-and-cats dataset to change the position and size of an apple but not to turn it into a cat (not even partially).</p> <p>It turns out that on a dataset with uniformly applied nuisance transformations, the nuisance transformations are valid style features: The result of randomly rotating an apple cannot be discerned as artificial when images of all classes, house cats and apples, were previously randomly rotated.</p> <p>Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> find that when they train a MUNIT network on a dataset with nuisance transformations and class imbalances, the MUNIT network decomposes the class and transformation distributions. The style latent space of the MUNIT network approximates the transformation distribution $T(\cdot |x)$. The content space preserves the remaining features of the image, such as its class. Thus, when translating an image, i.e., exchanging its style code, MUNIT applies a random nuisance transformation while preserving content. Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> use this method to generate data for classes with few examples. While the CNN is still unable to transfer invariance to $T(\cdot |x)$ between classes, it can now learn it for each class separately using the data generated by MUNIT, which has acquired knowledge of $T(\cdot |x)$ from the entire dataset (see also Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> section 4).</p> <p>So MUNIT decomposes the example-specific information, e.g., whether something is an apple or a house cat, from the meta-information, i.e., nuisance transformations applied to the entire dataset. When we add more classes, it has more data and can better learn the transformation distribution $T(\cdot |x)$. Does solving a meta-learning problem make MUNIT a meta-learner? Let’s look at the relationship MUNIT has with contemporary meta-learners</p> <h2 id="how-much-meta-learning-is-in-munit-">How much meta-learning is in MUNIT ?</h2> <p>To see how well MUNIT fits the definition of meta-learning, let’s see what the same survey papers we consulted earlier consider the structure of a meta-learning algorithm.</p> <h3 id="part-1-the-task-centered-view">Part 1: The task-centered view</h3> <p>Hospedales et al. [2021] <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite> defines a generic meta-learner as follows: An outer training loop with a set of trainable parameters iterates over tasks in a distribution of tasks. Formally a task is comprised of a dataset and a loss function $ \mathcal{T} = \{ \mathcal{D}, \mathcal{L} \} $. In an inner loop, a learning algorithm based on the outer loop’s parameters is instantiated for each task. We train it on a training set (<em>meta-training</em>) and test it on a validation set (<em>meta-validation</em>). We then use loss on this validation set to update the outer loop’s parameters. In this task-centered view of meta-learning, we can express the objective function as</p> <p> $$ \underset{\omega}{\mathrm{min}} \; \mathbb{E}_{\mathcal{T} \sim p(\mathcal{T})} \; \mathcal{L}(\mathcal{D}, \omega), $$ </p> <p>where $ \omega $ is parameters trained exclusively on the meta-level, i.e., the <em>meta-knowledge</em> learnable from the task distribution <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite>.</p> <p>This <em>meta-knowledge</em> is what the meta-learner accumulates and transfers across the tasks. Collecting meta-knowledge allows the meta-learner to improve its expected task performance with the number of tasks. The meta-knowledge in the experiment of Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> is the invariance to the nuisance transformations as the transformations are identical and need to be ignored for images of all classes. By creating additional transformed samples, the MUNIT network makes the meta-knowledge learnable for the CNN.</p> <p>The task-centered view of meta-learning brings us to a related issue: A meta-learner must discern and decompose task-specific knowledge from meta-knowledge. Contemporary meta-learners decompose meta-knowledge through the different objectives of their inner and outer loops and their respective loss terms. They store meta-knowledge in the outer loop’s parameter set $ \omega $ but must not learn task-specific information there. Any unlearned meta-features lead to slower adaptation, negatively impacting performance, <em>meta-underfitting</em>. On the other hand, any learned task-specific features will not generalize to unseen tasks in the distribution, thus also negatively impacting performance, <em>meta-overfitting</em>.</p> <p>We recall that, similarly, MUNIT <d-cite key="DBLP:conf/eccv/HuangLBK18"></d-cite> decomposes domain-specific style information and domain-agnostic content information. Applied to two domains, leopards and house cats, a MUNIT network will encode the domain-agnostic information, e.g., posture, scale, background, in its content latent space, and the domain-specific information, e.g., how a cat’s hair looks, in its style latent space. If the MUNIT network encoded the domain-agnostic information in the style latent space, the resulting image would not appear to be a good translation since the style information is discarded and replaced. It might turn a closed-eyed leopard into a staring cat. If the MUNIT network encoded the domain-specific transformation in the content latent space, the network would have difficulty translating between domains. A house cat might still have its original leopard fur.</p> <p>Although the single-domain application of MUNIT explicitly learns a single task and scales “over multiple data instances” instead of “multiple learning episodes”<d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite> it is clearly compatible with the task-centered view of meta-learning set forth <em>in the same survey paper</em>. Both meta-learning and multi-domain unsupervised image-to-image translation are thus learning problems that require a separation of the general from the specific.</p> <p>As we shall see, this is even visible when comparing their formalizations as optimization problems.</p> <h3 id="part-2-the-bi-level-programming-view">Part 2: The bi-level programming view</h3> <p>Francheschi et al. [2018] <d-cite key="DBLP:conf/icml/FranceschiFSGP18"></d-cite> show that all contemporary neural-network-based meta-learning approaches can be expressed as bi-level optimization problems. Formally the optimization objective of a general meta-learner can be expressed as:</p> <p> $$ \bbox[5pt, border: 2px solid blue]{ \begin{align*} \omega^{*} = \underset{\omega}{\mathrm{argmin}} \sum_{i=1}^{M} \mathcal{L}^{meta}(\theta^{* \; (i)}(\omega), D^{val}_i), \end{align*} } $$ </p> <p>where $M$ describes the number of tasks in a batch, $\mathcal{L}^{meta}$ is the meta-loss function, and $ D^{val}_i $ is the validation set of the task $ i $. $\omega$ represents the parameters exclusively updated in the outer loop. $ \theta^{* \; (i)} $ represents an inner loop learning a task that we can formally express as a sub-objective constraining the primary objective</p> <p> $$ \bbox[5pt, border: 2px solid red]{ \begin{align*} s.t. \; \theta^{* \; (i)} = \underset{\theta}{\mathrm{argmin}} \; \mathcal{L^{task}}(\theta, \omega, D^{tr}_i), \end{align*} } $$ </p> <p>where $ \theta $ are the model parameters updated in the inner loop, $ \mathcal{L}^{task} $ is the loss function by which they are updated and $ D^{tr}_i $ is the training set of the task $ i $ <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite>.</p> <p>While not adhering to Francheschi et al.’s [2018] notion of a meta-learner as “nesting two search problems”, it turns out that the loss functions of MUNIT can be similarly decomposed:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-1400.webp"/> <img src="/How-much-meta-learning-is-in-image-to-image-translation/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>MUNIT’s loss function consists of two adversarial (GAN) <d-cite key="DBLP:conf/nips/GoodfellowPMXWOCB14"></d-cite> loss terms (see figure above) with several auxiliary reconstruction loss terms. To keep the notation simple, we combine all reconstruction terms into a joined reconstruction loss $ \mathcal{L}_{recon}(\theta_c, \theta_s) $, where $ \theta_c $ are the parameters of the <em>content</em> encoding/decoding networks and $ \theta_s $ are the parameters of the <em>style</em> encoding/decoding networks. We will only look at one of the two GAN losses in detail since they are symmetric, and one is discarded entirely when MUNIT is used on a single domain in the fashion of Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite>.</p> <p>MUNIT’s GAN loss term is</p> <p> $$ \begin{align*} &amp;\mathcal{L}^{x_{2}}_{GAN}(\theta_d, \theta_c, \theta_s) \\\\ =&amp; \;\mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ +&amp; \;\mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right], \end{align*} $$ </p> <p>where the $ \theta_d $ represents the parameters of the discriminator network, $p(x_2)$ is the data of the second domain, $ c_1 $ is the content embedding of an image from the first domain to be translated. $ s_2 $ is a random style code of the second domain. $ D_2 $ is the discriminator of the second domain, and $ G_2 $ is its generator. MUNIT’s full objective function is:</p> <p> $$ \begin{align*} \underset{\theta_c, \theta_s}{\mathrm{argmin}} \; \underset{\theta_d}{\mathrm{argmax}}&amp; \;\mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ +&amp; \; \mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right], + \; \mathcal{L}^{x_{1}}_{GAN}(\theta_d, \theta_c, \theta_s) \\ +&amp; \;\mathcal{L}_{recon}(\theta_c, \theta_s) \end{align*} $$ </p> <p>(compare <d-cite key="DBLP:conf/eccv/HuangLBK18, DBLP:conf/nips/GoodfellowPMXWOCB14"></d-cite>). We can reformulate this into a bi-level optimization problem by extracting a minimization problem describing the update of the generative networks. We also drop the second GAN loss term as it is not relevant to our analysis.</p> <p> $$ \bbox[5px, border: 2px solid blue]{ \begin{align*} \omega^{*} &amp; = \{ \theta_c^*, \theta_s^* \} \\\\ &amp; = \underset{\theta_c, \theta_s}{\mathrm{argmin}} \; \mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d^{*})) \right] \\ &amp; + \mathcal{L}_{recon}(\theta_c, \theta_s), \end{align*} } $$ </p> <p>We then add a single constraint, a subsidiary maximization problem for the discriminator function:</p> <p> $$ \bbox[5px, border: 2px solid red]{ \begin{align*} &amp;s.t. \;\theta_d^{*} \\\\ &amp; = \underset{\theta_d}{\mathrm{argmax}} \; \mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ &amp; + \mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right] \end{align*} } $$ </p> <p>Interestingly, this bi-level view does not only resemble a meta-learning procedure as expressed above, but the bi-level optimization also facilitates a similar effect. Maximizing the discriminator’s performance in the constraint punishes style information encoded as content information. If style information is encoded as content information, the discriminator detects artifacts of the original domain in the translated image. Similarly, a meta-learner prevents <em>meta-overfitting</em> via an outer optimization loop.</p> <p><em>However, MUNIT, while representable as a bi-level optimization problem does not “essentially boil down to nesting two search problems”.<d-cite key="DBLP:conf/icml/FranceschiFSGP18"></d-cite></em> During GAN training, the discriminator’s parameters are updated through the changes in the generator’s parameters, which derive from the discriminator’s parameters, and so forth; The training of the discriminator and generator are dependent processes. Crucially, they depend on each other symmetrically, forming a min-max game. Contemporary meta-learners, meanwhile, are strictly hierarchical, with an outer and inner optimization loop.</p> <h3 id="now-does-munit-meta-learn">Now, does MUNIT meta-learn?</h3> <p>So it appears that while not conforming to any verbal definition of a contemporary meta-learner MUNIT seems to:</p> <p>a) adhere to multiple formalizations made in the very same publications to define meta-learning</p> <p>b) solve a meta-learning problem via GIT when applied to a single domain (if you agree with the conclusion of the previous chapter)</p> <p>We thus conclude:</p> <p>When applied to a single domain MUNIT <em>does</em> meta-learn as it combines information from all classes to extract the transformation distribution. While it does not perform classification explicitly, the class information of an image is encoded in MUNIT’s content space. Since MUNIT is trained in an unsupervised way, it is probably closer to a distance metric than an actual class label. We might thus classify single-domain MUNIT as an unsupervised, generative meta-learner.</p> <h2 id="implications">Implications</h2> <p>That invariance transfer and GIT are meta-learning and that MUNIT is a meta-learner is important. Granted, it is not especially hard to see that invariance transfer is a form of “learning to learn” or that Image-to-Image translation is essentially a mechanism to decompose class-specific form general features.</p> <p>However, because contemporary meta-learning has been narrowly cast as “improving a learning algorithm over multiple learning episodes”<d-cite key="DBLP:journals/pami/HospedalesAMS22"> and "nesting two search problems"<d-cite key="DBLP:conf/icml/FranceschiFSGP18"> it is hard to recognize GIT as meta-learning.</d-cite></d-cite></p> <p>In these authors opinion this is not GIT’s fault, but a sign that meta-learning has recently been conceived of too narrowly. Zhou et al.’s [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiment is a beautiful illustration of this showing that something as general as a GAN loss term, with appropriate modifications, can be used to meta-learn.</p> <p>A too-narrow conception goes further than obscuring some experiment’s significance though: Meta-learning as a field has recently struggled to compete with less specialized architectures<d-cite key="DBLP:journals/corr/abs-2104-02638"></d-cite><d-cite key="DBLP:journals/corr/abs-1904-04232"></d-cite>. Multi-task datasets are hard to scale <d-cite key="triantafillou2019meta"></d-cite>, as are episode rollouts <d-cite key="DBLP:conf/icml/FinnAL17"></d-cite>. Meanwhile, large-scale architectures have shown impressive zero-shot capabilities<d-cite key="dosovitskiy2021an"></d-cite><d-cite key="pmlr-v139-radford21a"></d-cite>.</p> <p>Zhou et al.’s [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> contributions are therefore important as a challenge to the status quo in meta-learning. MUNIT seems to meta-learn by embedding class (and class-specific features) in one space and transformation-specific features (e.g., how bright/dark) in another. This seems to point to a conception of meta-learning as finding mutual information between sets of examples (not necessarily defined by class or transformation feature but by arbitrary concepts) or hierarchies of such sets. Examining and designing mechanisms by which such behavior can be evoked is an exciting direction for future work.</p> <h2 id="key-takeaways">Key Takeaways</h2> <ol> <li> <p>Zhou et al.’s <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiments show that the meta-learning setting can be formulated more broadly than learning an explicit task distribution, suggesting that specialized datasets are not necessary.</p> </li> <li> <p>Using GIT, Zhou et al. <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> show that meta-learning algorithms can come in shapes other than inner and outer training loops. Analysis suggests that countervailing loss terms facilitate the decomposition of meta-features from task-specific features.</p> </li> <li> <p>Our discussion of Zhou et al.’s <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiments suggests, that when thinking about meta-learning, thinking about mutual information between batches of examples (not necessarily aligned with class labels) and how to extract it trumps thinking about distinct tasks.</p> </li> </ol>]]></content><author><name>Maximilian Eißler</name></author><summary type="html"><![CDATA[...in which we find a connection between meta-learning literature and a paper studying how well CNNs deal with nuisance transforms in a class-imbalanced setting. Closer inspection reveals a surprising amount of similarity - from meta-information to loss functions. This implies that the current conception of meta-learning might be too narrow.]]></summary></entry></feed>